{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32db6c6a",
   "metadata": {},
   "source": [
    "### Importing needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3977cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pdb import set_trace\n",
    "import requests\n",
    "import os \n",
    "import json\n",
    "from typing import Dict\n",
    "from time import sleep\n",
    "import random\n",
    "import bs4\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "#pip install bs4\n",
    "#pip install dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae29686",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6eab89",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3d2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting the URL of the places listed in the first 400 pages \n",
    "\n",
    "class urlDownloader():\n",
    "    WEBSITE = \"https://www.atlasobscura.com/places?page={}&sort=likes_count\"\n",
    "\n",
    "    def __init__(self,number_of_pages:int=400) -> None:\n",
    "        self.number_of_pages = number_of_pages\n",
    "    \n",
    "    #Retriving Title\n",
    "    def _retriveTitle(self,soup_tags:bs4.element.Tag) -> str:\n",
    "        title =soup_tags.find(\"h3\").text\n",
    "        if title is not None:\n",
    "            return title\n",
    "        return \"\"\n",
    "    \n",
    "    #Retriving Link \n",
    "    def _retrieveLink(self,soup_tags:bs4.element.Tag) -> str: \n",
    "        link = soup_tags.find(\"a\").attrs[\"href\"]\n",
    "        if link is not None:\n",
    "            return link\n",
    "        return \"\"\n",
    "   \n",
    "    def saver_format(self,data:List[tuple],format:str=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            format (str, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "        data = pd.DataFrame(data,columns=[\"NAME\",\"URL\"])\n",
    "        data[\"URL\"] = \"https://www.atlasobscura.com\" + data.URL\n",
    "        if format == \"csv\":\n",
    "            data.to_csv(\"datas/urls.csv\",index=False)\n",
    "        if format == \"txt\":\n",
    "            with open(\"datas/urls.txt\",\"w\") as f:\n",
    "                for idx,row in data.iterrows():\n",
    "                    f.write(\" \".join(row.values)+\"\\n\")\n",
    "\n",
    "    def download_urls(self) -> pd.DataFrame:\n",
    "        data_holder:List[tuple] = [] \n",
    "    \n",
    "        for page_number in range(1,self.number_of_pages+1):\n",
    "            sleep(random.randint(0,2))  #to prevent blocking, because of many requests.\n",
    "            data = requests.get(urlDownloader.WEBSITE.format(page_number))\n",
    "            soup = bs4.BeautifulSoup(data.content,features=\"lxml\")\n",
    "            roi_places = soup.find_all(\"div\", {\"class\": \"col-md-4 col-sm-6 col-xs-12\"})#18\n",
    "            for place in roi_places: \n",
    "                place_title = self._retriveTitle(place)\n",
    "                place_link= self._retrieveLink(place)\n",
    "                data_holder.append((place_title,place_link))\n",
    "        self.saver_format(data=data_holder,format=\"csv\")\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    downloader = urlDownloader(number_of_pages=400)\n",
    "    downloader.download_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6305c",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mount Moriah Cemetery\n",
      "Downloading New Orleans Historic Voodoo Museum\n",
      "Downloading The Neon Museum\n",
      "Downloading Museum of Vampires \n",
      "Downloading Wisteria Tunnel\n",
      "Downloading Explorers Club Headquarters\n",
      "Downloading Mapparium\n",
      "Downloading House of Eternal Return\n",
      "Downloading Sagano Bamboo Forest\n",
      "Downloading The Viktor Wynd Museum of Curiosities, Fine Art & Natural History\n",
      "Downloading 'Akhob'\n",
      "Downloading Paint Mines Interpretive Park\n",
      "Downloading Puzzlewood\n",
      "Downloading Hamilton Pool\n",
      "Downloading Abandoned Jazzland\n",
      "Downloading Fairy Pools\n",
      "Downloading 5 Beekman Street\n",
      "Downloading Havasupai Falls\n",
      "Downloading Rakotzbrücke Devil's Bridge\n",
      "Downloading Lexington Candy Shop \n",
      "Downloading Horseshoe Bend\n",
      "Downloading The Hardy Tree\n",
      "Downloading Electric Ladyland: The Museum of Fluorescent Art\n",
      "Downloading Cat Island\n",
      "Downloading Sunken City \n",
      "Downloading Alcatraz Island\n",
      "Downloading Père Lachaise Cemetery\n",
      "Downloading Yayoi Kusama Firefly Infinity Mirror Room\n",
      "Downloading The Witch House of Salem\n",
      "Downloading Dry Tortugas\n",
      "Downloading Please Don't Tell\n",
      "Downloading Bradbury Building\n",
      "Downloading Albion Castle\n",
      "Downloading Griffith Observatory's Tesla Coil\n",
      "Downloading The Museum of Alchemists and Magicians of Old Prague\n",
      "Downloading House on the Rock\n",
      "Downloading Mysterious Bookshop\n",
      "Downloading Fairy Glen\n",
      "Downloading Track 61\n",
      "Downloading Hanging Lake\n",
      "Downloading The SeaGlass Carousel\n",
      "Downloading Berliner Unterwelten (Subterranean Berlin)\n",
      "Downloading Old Zoo Picnic Area\n",
      "Downloading Fremont Troll\n",
      "Downloading The 'Ghostbusters' Firehouse\n",
      "Downloading Philadelphia's Magic Gardens\n",
      "Downloading Synchronous Fireflies of the Great Smoky Mountains\n",
      "Downloading Museum of Death\n",
      "Downloading Bronson Cave\n",
      "Downloading Petite Ceinture\n",
      "Downloading Santa Maria della Concezione Crypts\n",
      "Downloading New York's Hidden Tropical Forest\n",
      "Downloading Platform 9 3/4\n",
      "ERROR [Errno 2] No such file or directory: 'datas/htmls/Platform 9 3/4.html'\n",
      "Downloading Peephole Cinema\n",
      "Downloading Seattle Underground \n",
      "Downloading Tannen's Magic Shop\n",
      "Downloading Bibliothèque nationale de France (National Library of France)\n",
      "Downloading 59 Rivoli\n",
      "Downloading The High Line\n",
      "Downloading Valle dei Mulini (Valley of the Mills)\n",
      "Downloading Oz Park\n",
      "Downloading Venetian Pool\n",
      "Downloading Monster Kabinett\n",
      "Downloading Horsetail Fall's Firefall\n",
      "Downloading Liquidrom\n",
      "Downloading Trinity Place Bank Vault Bar\n",
      "Downloading Multnomah Falls\n",
      "Downloading President Heads\n",
      "Downloading White Sands National Park\n",
      "Downloading Obscura Antiques and Oddities\n",
      "Downloading Ah-Shi-Sle-Pah Wilderness Study Area\n",
      "Downloading Warren Anatomical Museum\n",
      "Downloading Hollywood Forever Cemetery\n",
      "Downloading Marie Laveau's Tomb\n",
      "Downloading The Stanley Hotel\n",
      "Downloading Mesa Verde National Park\n",
      "Downloading Ye Olde Curiosity Shop\n",
      "Downloading Magic Circle Museum\n",
      "Downloading International Church of Cannabis\n",
      "Downloading Jean Lafitte's Old Absinthe House\n",
      "Downloading Phantasma Gloria\n",
      "Downloading Cecil Court\n",
      "Downloading Goodwin's Court\n",
      "Downloading Watts Towers \n",
      "Downloading Lost Sea\n",
      "Downloading Woolly Mammoth Antiques and Oddities\n",
      "Downloading Tiffany Dome\n",
      "Downloading Nieuwe Spiegelstraat\n",
      "Downloading Museum of Sex\n",
      "Downloading Galco's Soda Pop Stop\n",
      "Downloading Forest Hills Cemetery\n",
      "Downloading Garden of Oz\n",
      "Downloading Shakespeare and Company\n",
      "Downloading Shofuso Japanese House and Garden\n",
      "Downloading Hall of Mosses\n",
      "Downloading Roosevelt Island Cat Sanctuary\n",
      "Downloading House of Nicolas Flamel\n",
      "Downloading Cave of Kelpius\n",
      "Downloading Blue Lagoon\n",
      "Downloading Montmartre Cemetery\n",
      "Downloading Torre Argentina (Roman Cat Sanctuary)\n",
      "Downloading Nine Mile Canyon\n",
      "Downloading Marie Laveau's House of Voodoo\n",
      "Downloading Neuschwanstein Castle\n",
      "Downloading Wood Line\n",
      "Downloading Berlin Botanical Garden\n",
      "Downloading Doll's Head Trail\n",
      "Downloading The Singing Oak \n",
      "Downloading Jigokudani Monkey Park\n",
      "Downloading Museum of the Weird\n",
      "Downloading Labyrinth at Land’s End\n",
      "Downloading Museum of Pop Culture\n",
      "Downloading The Warren's Occult Museum \n",
      "Downloading The Bell Witch Cave\n",
      "Downloading Mary King's Close\n",
      "Downloading Centralia\n",
      "Downloading Abandoned Virginia Renaissance Faire \n",
      "Downloading National Capitol Columns\n",
      "Downloading The Ramble Cave\n",
      "Downloading 221b Baker Street\n",
      "Downloading The Monsters of Bomarzo\n",
      "Downloading The Churchill War Rooms\n",
      "Downloading Hollywood Sign\n",
      "Downloading Portland's Shanghai Tunnels\n",
      "Downloading The Exorcist Stairs\n",
      "Downloading Ennis House\n",
      "Downloading Crater Lake\n",
      "Downloading Audium Theatre of Sound-Sculptured Space\n",
      "Downloading The Treasures in the Trash Collection\n",
      "Downloading Fifty-Two 80's\n",
      "Downloading Edgar Allan Poe National Historic Site\n",
      "Downloading California Institute of Abnormalarts (CIA)\n",
      "Downloading The Museum of Interesting Things\n",
      "Downloading Catacombs of Washington, D.C.\n",
      "Downloading Kensal Green Cemetery and Catacombs\n",
      "Downloading Chinatown Ice Cream Factory\n",
      "Downloading Brooklyn Superhero Supply Store\n",
      "Downloading Wicker Park Secret Agent Supply Co.\n",
      "Downloading Banksy's 'Designated Graffiti Area'\n",
      "Downloading Alnwick Poison Garden\n",
      "Downloading Mystery Soda Machine\n",
      "Downloading Nutshell Studies of Unexplained Death\n",
      "Downloading National Atomic Testing Museum\n",
      "Downloading Japanese Tea Garden \n",
      "Downloading Rue Crémieux\n",
      "Downloading The Freakybuttrue Peculiarium\n",
      "Downloading Machu Picchu: The Lost City of The Inca\n",
      "Downloading Saint Louis Cemetery No. 1\n",
      "Downloading Necromance\n",
      "Downloading Mont Saint-Michel\n",
      "Downloading Loved to Death\n",
      "Downloading San Francisco Conservatory of Flowers\n",
      "Downloading The Narrows\n",
      "Downloading Montezuma Castle\n",
      "Downloading The Sonorous Stones of Ringing Rocks Park\n",
      "Downloading Garfield Park Conservatory\n",
      "Downloading Tajikistan Tearoom\n",
      "Downloading Museum of Neon Art\n",
      "Downloading International Cryptozoology Museum\n",
      "Downloading Willis Tower Glass Platform\n",
      "Downloading No Vacancy\n",
      "Downloading Vampire Café\n",
      "Downloading Svartifoss\n",
      "Downloading Glass Beach\n",
      "Downloading MoMath - The Museum of Mathematics\n"
     ]
    }
   ],
   "source": [
    "class htmlDownloader():\n",
    "    def __init__(self,file_path:str=\"datas/urls.csv\") -> None:\n",
    "        self.urls = self._url_reader(file_path)\n",
    "\n",
    "    def _url_reader(self,path:str,format_file=\"csv\") -> pd.DataFrame :\n",
    "        if format_file ==\"txt\" :\n",
    "            return pd.read_csv(path,sep=\" \")\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "    #if program stops, to not lose the data that collected up to the stopping point.\n",
    "    def download_htmls(self) -> None :\n",
    "        config = self.last_download_control()\n",
    "        roi_urls = self.urls.iloc[config[\"last_downloaded\"]:]\n",
    "        for idx,row in roi_urls.iterrows():\n",
    "            try:\n",
    "                print(f\"Downloading {row.NAME}\")\n",
    "                sleep(random.randint(0,2))\n",
    "                htmls = requests.get(row.URL).content\n",
    "                saving_path = f\"datas/htmls/{row.NAME}.html\"\n",
    "                with open(saving_path,\"wb\") as f:\n",
    "                    f.write(htmls)\n",
    "                config[\"last_downloaded\"] += 1 \n",
    "\n",
    "                if idx % 10 == 0 :\n",
    "                    self.save_config_file(config)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR {e}\")\n",
    "                self.save_config_file(config=config)\n",
    "                    \n",
    "\n",
    "    #if program stops, to not lose last data that collected up to the stopping point.\n",
    "    def last_download_control(self) -> Dict[str,int]:\n",
    "\n",
    "        if os.path.isfile(\"config.json\"):\n",
    "            with open(\"config.json\",\"rb\") as f :\n",
    "                config = json.load(f)\n",
    "        else:\n",
    "            config = {\"last_downloaded\":0}\n",
    "            with open(\"config.json\",\"w\") as f:\n",
    "                json.dump(config,f)\n",
    "        return config\n",
    "\n",
    "    def save_config_file(self,config:Dict[str,int]):\n",
    "        with open(\"config.json\",\"w\") as f:\n",
    "            json.dump(config,f)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    downloader = htmlDownloader()\n",
    "    downloader.download_htmls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab0be5",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class htmlParser():\n",
    "    HTML_FILES = \"datas/htmls/\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.html_files = os.listdir(htmlParser.HTML_FILES)\n",
    "        self.dataframe = pd.read_csv(\"datas/urls.csv\")\n",
    "        self.dataframe.NAME = self.dataframe.NAME.str.strip()\n",
    "        self.process_pages()\n",
    "\n",
    "\n",
    "    def read_file(self,file_name:str) -> bs4.BeautifulSoup:\n",
    "        file_path = htmlParser.HTML_FILES + file_name\n",
    "\n",
    "        with open(file_path,\"r\") as f:\n",
    "            html = f.read()\n",
    "        return bs4.BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "\n",
    "    def extract_header_information(self,soup:bs4.BeautifulSoup):\n",
    "        roi_header = soup.find_all(\"div\", {\"class\": \"DDPage__header-container grid-row\"})[0]\n",
    "        self.placeName = roi_header.find(\"h1\", {\"class\":\"DDPage__header-title\"}).text.strip()\n",
    "\n",
    "        location = roi_header.find(\"div\", {\"class\":\"DDPage__header-place-location\"}).text.strip()\n",
    "\n",
    "        counters = roi_header.find_all(\"div\",{\"class\":\"title-md item-action-count\"})\n",
    "        self.numPeopleVisited,self.numPeopleWant = int(counters[0].text.strip()),int(counters[1].text.strip())\n",
    "        self.placeShortDesc = roi_header.find(\"h3\", {\"class\":\"DDPage__header-dek\"}).text.strip()\n",
    "        # set_trace(header=\"header infos\")\n",
    "    \n",
    "    def extract_descriptions(self,soup:bs4.BeautifulSoup):\n",
    "\n",
    "        ## Main Descriptions \n",
    "        placeDesc = soup.find_all(\"div\", {\"class\": \"DDP__body-copy\"})[0]\n",
    "        self.placeDesc = \"\".join([p.text.strip() for p in placeDesc.find_all(\"p\")])\n",
    "        # set_trace(header=\"descriptions\")\n",
    "    \n",
    "    def extract_sidebar(self,soup:bs4.BeautifulSoup):\n",
    "        sidebar = soup.find(\"div\", {\"class\":\"DDPageSiderail\"})\n",
    "        nearby_places = sidebar.find_all(\"div\",{\"class\":\"DDPageSiderailRecirc__item-title\"})\n",
    "        positions =  sidebar.find(\"div\",{\"class\":\"DDPageSiderail__coordinates js-copy-coordinates\"}).attrs[\"data-coordinates\"].split(\",\")\n",
    "        \n",
    "        #9-Latitud and Longitude of the place's location,  #7-placeNearby,  8-placeAddress\n",
    "        self.placeAlt, self.placeLong = float(positions[0].strip()), float(positions[1].strip())\n",
    "        self.placeNearby = [place.text for place in nearby_places]\n",
    "        self.placeAddress =  sidebar.find(\"address\",{\"class\":\"DDPageSiderail__address\"}).find(\"div\",recursive=False).get_text(\" \").replace(\"\\n\",\"\")\n",
    "\n",
    "    def extract_footer(self,soup:bs4.BeautifulSoup):\n",
    "        footer = soup.find(\"div\", {\"id\":\"ugc-module\"})\n",
    "        Editors = footer.find_all(\"a\",{\"class\":\"DDPContributorsList__contributor\"})\n",
    "        placePubDate = footer.find(\"div\",{\"class\":\"DDPContributor__name\"}).text\n",
    "        #10-placeEditors, 11-placePubDate, 2-placeTags\n",
    "        self.placeEditors = [editors.find(\"span\").text if editors.find(\"span\") else editors.text for editors in Editors]\n",
    "        self.placePubDate = datetime.strptime(placePubDate,\"%B %d, %Y\")\n",
    "        self.placeTags = [item.get_text(\"\").replace(\"\\n\",\"\") for item in soup.find_all(\"a\",{\"class\":\"itemTags__link js-item-tags-link\"})]\n",
    "        # set_trace(header=\"sidebar\")\n",
    "\n",
    "    def extract_related_places(self,soup:bs4.BeautifulSoup):\n",
    "        \n",
    "        related_list = soup.find(\"div\",{\n",
    "            \"class\":\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\",\n",
    "            \"data-gtm-template\":\"DDP Footer Recirc Lists\"\n",
    "        })\n",
    "        related_places = soup.find(\"div\",{\n",
    "            \"class\":\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\",\n",
    "            \"data-gtm-template\":\"DDP Footer Recirc Related\"\n",
    "        })\n",
    "        #12-placeRelatedLists,   #13-placeRelatedPlaces\n",
    "        self.placeRelatedLists = [data.find(\"span\").text.strip() for data in related_list.find_all(\"h3\")]\n",
    "        self.placeRelatedPlaces = [data.find(\"span\").text.strip() for  data in related_places.find_all(\"h3\")]\n",
    "        \n",
    "    def process_pages(self):\n",
    "        \n",
    "        for idx,file_name in enumerate(self.html_files):\n",
    "            try:\n",
    "                soup = self.read_file(file_name=file_name)\n",
    "                self.extract_header_information(soup=soup)\n",
    "                self.extract_descriptions(soup=soup)\n",
    "                self.extract_sidebar(soup=soup)\n",
    "                self.extract_footer(soup=soup)\n",
    "                self.extract_related_places(soup=soup)\n",
    "                #14-placeUrl\n",
    "                self.placeUrl = self.dataframe.loc[self.dataframe.NAME == self.placeName][\"URL\"]\n",
    "                self.save_tsv_file(idx)\n",
    "            except Exception as e : \n",
    "                print(e)\n",
    "                set_trace() #debuggig.\n",
    "                \n",
    "    def save_tsv_file(self,idx:int):\n",
    "        with open(f\"datas/tsv_files/place_{idx}.tsv\",\"w\") as f:\n",
    "            whole_data = [\n",
    "                self.placeName,str(self.placeTags),str(self.numPeopleVisited),\n",
    "                str(self.numPeopleWant),self.placeDesc,self.placeShortDesc,\n",
    "                str(self.placeNearby),self.placeAddress,str(self.placeAlt),\n",
    "                str(self.placeLong),str(self.placeEditors),self.placePubDate.strftime(\"%Y-%m-%d\"),\n",
    "                str(self.placeRelatedLists),\n",
    "                str(self.placeRelatedPlaces),self.placeUrl.values[0]]\n",
    "            \n",
    "            tsv = \"\\t\".join(whole_data)\n",
    "            f.write(tsv)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    htmlParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0392f2",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bba640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "126e8677",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952459da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bcd65f8",
   "metadata": {},
   "source": [
    "###  2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d9d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f180f38f",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cf27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d42f345",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eba25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c4220e6",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "254508be",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9fd3eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42c20f18",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ca8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4e27823",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b17b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9fdf3e5",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705498ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8679609a",
   "metadata": {},
   "source": [
    "## 5. BONUS: More complex search engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760cfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c0bb78",
   "metadata": {},
   "source": [
    "## 6. Command line question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9bf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "358090cf",
   "metadata": {},
   "source": [
    "## 7. Theoretical question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
