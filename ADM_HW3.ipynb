{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02182b5e",
   "metadata": {},
   "source": [
    "### Importing needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7dbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pdb import set_trace\n",
    "import requests\n",
    "import os \n",
    "import json\n",
    "from typing import Dict\n",
    "from time import sleep\n",
    "import random\n",
    "import bs4\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "#pip install bs4\n",
    "#pip install dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f618fdc",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6eab89",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3d2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting the URL of the places listed in the first 400 pages \n",
    "\n",
    "class urlDownloader():\n",
    "    WEBSITE = \"https://www.atlasobscura.com/places?page={}&sort=likes_count\"\n",
    "\n",
    "    def __init__(self,number_of_pages:int=400) -> None:\n",
    "        self.number_of_pages = number_of_pages\n",
    "    \n",
    "    #Retriving Title\n",
    "    def _retriveTitle(self,soup_tags:bs4.element.Tag) -> str:\n",
    "        title =soup_tags.find(\"h3\").text\n",
    "        if title is not None:\n",
    "            return title\n",
    "        return \"\"\n",
    "    \n",
    "    #Retriving Link \n",
    "    def _retrieveLink(self,soup_tags:bs4.element.Tag) -> str: \n",
    "        link = soup_tags.find(\"a\").attrs[\"href\"]\n",
    "        if link is not None:\n",
    "            return link\n",
    "        return \"\"\n",
    "   \n",
    "    def saver_format(self,data:List[tuple],format:str=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            format (str, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "        data = pd.DataFrame(data,columns=[\"NAME\",\"URL\"])\n",
    "        data[\"URL\"] = \"https://www.atlasobscura.com\" + data.URL\n",
    "        if format == \"csv\":\n",
    "            data.to_csv(\"datas/urls.csv\",index=False)\n",
    "        if format == \"txt\":\n",
    "            with open(\"datas/urls.txt\",\"w\") as f:\n",
    "                for idx,row in data.iterrows():\n",
    "                    f.write(\" \".join(row.values)+\"\\n\")\n",
    "\n",
    "    def download_urls(self) -> pd.DataFrame:\n",
    "        data_holder:List[tuple] = [] \n",
    "    \n",
    "        for page_number in range(1,self.number_of_pages+1):\n",
    "            sleep(random.randint(0,2))  #to prevent blocking, because of many requests.\n",
    "            data = requests.get(urlDownloader.WEBSITE.format(page_number))\n",
    "            soup = bs4.BeautifulSoup(data.content,features=\"lxml\")\n",
    "            roi_places = soup.find_all(\"div\", {\"class\": \"col-md-4 col-sm-6 col-xs-12\"})#18\n",
    "            for place in roi_places: \n",
    "                place_title = self._retriveTitle(place)\n",
    "                place_link= self._retrieveLink(place)\n",
    "                data_holder.append((place_title,place_link))\n",
    "        self.saver_format(data=data_holder,format=\"csv\")\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    downloader = urlDownloader(number_of_pages=400)\n",
    "    downloader.download_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf497b3",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518ba04",
   "metadata": {},
   "source": [
    "First we downloaded the HTML, corresponding to each of the collected URLs.\n",
    "When we collect a single page, immediately save its HTML in the folder datas/htmls/ to not lose the data collected. \n",
    "\n",
    "We saved all downloaded HTML pages into spesific folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mount Moriah Cemetery\n",
      "Downloading New Orleans Historic Voodoo Museum\n",
      "Downloading The Neon Museum\n",
      "Downloading Museum of Vampires \n",
      "Downloading Wisteria Tunnel\n",
      "Downloading Explorers Club Headquarters\n",
      "Downloading Mapparium\n",
      "Downloading House of Eternal Return\n",
      "Downloading Sagano Bamboo Forest\n",
      "Downloading The Viktor Wynd Museum of Curiosities, Fine Art & Natural History\n",
      "Downloading 'Akhob'\n",
      "Downloading Paint Mines Interpretive Park\n",
      "Downloading Puzzlewood\n",
      "Downloading Hamilton Pool\n",
      "Downloading Abandoned Jazzland\n",
      "Downloading Fairy Pools\n",
      "Downloading 5 Beekman Street\n",
      "Downloading Havasupai Falls\n",
      "Downloading Rakotzbrücke Devil's Bridge\n",
      "Downloading Lexington Candy Shop \n",
      "Downloading Horseshoe Bend\n",
      "Downloading The Hardy Tree\n",
      "Downloading Electric Ladyland: The Museum of Fluorescent Art\n",
      "Downloading Cat Island\n",
      "Downloading Sunken City \n",
      "Downloading Alcatraz Island\n",
      "Downloading Père Lachaise Cemetery\n",
      "Downloading Yayoi Kusama Firefly Infinity Mirror Room\n",
      "Downloading The Witch House of Salem\n",
      "Downloading Dry Tortugas\n",
      "Downloading Please Don't Tell\n",
      "Downloading Bradbury Building\n",
      "Downloading Albion Castle\n",
      "Downloading Griffith Observatory's Tesla Coil\n",
      "Downloading The Museum of Alchemists and Magicians of Old Prague\n",
      "Downloading House on the Rock\n",
      "Downloading Mysterious Bookshop\n",
      "Downloading Fairy Glen\n",
      "Downloading Track 61\n",
      "Downloading Hanging Lake\n",
      "Downloading The SeaGlass Carousel\n",
      "Downloading Berliner Unterwelten (Subterranean Berlin)\n",
      "Downloading Old Zoo Picnic Area\n",
      "Downloading Fremont Troll\n",
      "Downloading The 'Ghostbusters' Firehouse\n",
      "Downloading Philadelphia's Magic Gardens\n",
      "Downloading Synchronous Fireflies of the Great Smoky Mountains\n",
      "Downloading Museum of Death\n",
      "Downloading Bronson Cave\n",
      "Downloading Petite Ceinture\n",
      "Downloading Santa Maria della Concezione Crypts\n",
      "Downloading New York's Hidden Tropical Forest\n",
      "Downloading Platform 9 3/4\n",
      "ERROR [Errno 2] No such file or directory: 'datas/htmls/Platform 9 3/4.html'\n",
      "Downloading Peephole Cinema\n",
      "Downloading Seattle Underground \n",
      "Downloading Tannen's Magic Shop\n",
      "Downloading Bibliothèque nationale de France (National Library of France)\n",
      "Downloading 59 Rivoli\n",
      "Downloading The High Line\n",
      "Downloading Valle dei Mulini (Valley of the Mills)\n",
      "Downloading Oz Park\n",
      "Downloading Venetian Pool\n",
      "Downloading Monster Kabinett\n",
      "Downloading Horsetail Fall's Firefall\n",
      "Downloading Liquidrom\n",
      "Downloading Trinity Place Bank Vault Bar\n",
      "Downloading Multnomah Falls\n",
      "Downloading President Heads\n",
      "Downloading White Sands National Park\n",
      "Downloading Obscura Antiques and Oddities\n",
      "Downloading Ah-Shi-Sle-Pah Wilderness Study Area\n",
      "Downloading Warren Anatomical Museum\n",
      "Downloading Hollywood Forever Cemetery\n",
      "Downloading Marie Laveau's Tomb\n",
      "Downloading The Stanley Hotel\n",
      "Downloading Mesa Verde National Park\n",
      "Downloading Ye Olde Curiosity Shop\n",
      "Downloading Magic Circle Museum\n",
      "Downloading International Church of Cannabis\n",
      "Downloading Jean Lafitte's Old Absinthe House\n",
      "Downloading Phantasma Gloria\n",
      "Downloading Cecil Court\n",
      "Downloading Goodwin's Court\n",
      "Downloading Watts Towers \n",
      "Downloading Lost Sea\n",
      "Downloading Woolly Mammoth Antiques and Oddities\n",
      "Downloading Tiffany Dome\n",
      "Downloading Nieuwe Spiegelstraat\n",
      "Downloading Museum of Sex\n",
      "Downloading Galco's Soda Pop Stop\n",
      "Downloading Forest Hills Cemetery\n",
      "Downloading Garden of Oz\n",
      "Downloading Shakespeare and Company\n",
      "Downloading Shofuso Japanese House and Garden\n",
      "Downloading Hall of Mosses\n",
      "Downloading Roosevelt Island Cat Sanctuary\n",
      "Downloading House of Nicolas Flamel\n",
      "Downloading Cave of Kelpius\n",
      "Downloading Blue Lagoon\n",
      "Downloading Montmartre Cemetery\n",
      "Downloading Torre Argentina (Roman Cat Sanctuary)\n",
      "Downloading Nine Mile Canyon\n",
      "Downloading Marie Laveau's House of Voodoo\n",
      "Downloading Neuschwanstein Castle\n",
      "Downloading Wood Line\n",
      "Downloading Berlin Botanical Garden\n",
      "Downloading Doll's Head Trail\n",
      "Downloading The Singing Oak \n",
      "Downloading Jigokudani Monkey Park\n",
      "Downloading Museum of the Weird\n",
      "Downloading Labyrinth at Land’s End\n",
      "Downloading Museum of Pop Culture\n",
      "Downloading The Warren's Occult Museum \n",
      "Downloading The Bell Witch Cave\n",
      "Downloading Mary King's Close\n",
      "Downloading Centralia\n",
      "Downloading Abandoned Virginia Renaissance Faire \n",
      "Downloading National Capitol Columns\n",
      "Downloading The Ramble Cave\n",
      "Downloading 221b Baker Street\n",
      "Downloading The Monsters of Bomarzo\n",
      "Downloading The Churchill War Rooms\n",
      "Downloading Hollywood Sign\n",
      "Downloading Portland's Shanghai Tunnels\n",
      "Downloading The Exorcist Stairs\n",
      "Downloading Ennis House\n",
      "Downloading Crater Lake\n",
      "Downloading Audium Theatre of Sound-Sculptured Space\n",
      "Downloading The Treasures in the Trash Collection\n",
      "Downloading Fifty-Two 80's\n",
      "Downloading Edgar Allan Poe National Historic Site\n",
      "Downloading California Institute of Abnormalarts (CIA)\n",
      "Downloading The Museum of Interesting Things\n",
      "Downloading Catacombs of Washington, D.C.\n",
      "Downloading Kensal Green Cemetery and Catacombs\n",
      "Downloading Chinatown Ice Cream Factory\n",
      "Downloading Brooklyn Superhero Supply Store\n",
      "Downloading Wicker Park Secret Agent Supply Co.\n",
      "Downloading Banksy's 'Designated Graffiti Area'\n",
      "Downloading Alnwick Poison Garden\n",
      "Downloading Mystery Soda Machine\n",
      "Downloading Nutshell Studies of Unexplained Death\n",
      "Downloading National Atomic Testing Museum\n",
      "Downloading Japanese Tea Garden \n",
      "Downloading Rue Crémieux\n",
      "Downloading The Freakybuttrue Peculiarium\n",
      "Downloading Machu Picchu: The Lost City of The Inca\n",
      "Downloading Saint Louis Cemetery No. 1\n",
      "Downloading Necromance\n",
      "Downloading Mont Saint-Michel\n",
      "Downloading Loved to Death\n",
      "Downloading San Francisco Conservatory of Flowers\n",
      "Downloading The Narrows\n",
      "Downloading Montezuma Castle\n",
      "Downloading The Sonorous Stones of Ringing Rocks Park\n",
      "Downloading Garfield Park Conservatory\n",
      "Downloading Tajikistan Tearoom\n",
      "Downloading Museum of Neon Art\n",
      "Downloading International Cryptozoology Museum\n",
      "Downloading Willis Tower Glass Platform\n",
      "Downloading No Vacancy\n",
      "Downloading Vampire Café\n",
      "Downloading Svartifoss\n",
      "Downloading Glass Beach\n",
      "Downloading MoMath - The Museum of Mathematics\n",
      "Downloading Lake Shawnee Amusement Park\n",
      "Downloading Mosaic Tile House\n",
      "Downloading Devils Tower\n",
      "Downloading Brattle Book Shop\n",
      "Downloading Good Vibrations Antique Vibrator Museum\n",
      "Downloading The Grotto\n",
      "Downloading Strahov Monastery \n",
      "Downloading Manatee Springs\n",
      "Downloading Clifton's Cafeteria\n",
      "Downloading Musée des Arts Forains \n",
      "Downloading Novelty Automation\n",
      "Downloading Isabella Stewart Gardner Museum (The Gardner)\n",
      "Downloading Institute of Illegal Images\n",
      "Downloading Wagner Free Institute\n",
      "Downloading  International Spy Museum\n",
      "Downloading I Love You Wall\n",
      "Downloading Bodega \n",
      "Downloading Preserved Remnants of 17th Century New York\n",
      "Downloading Goldbar\n",
      "Downloading Winfield Street Slides \n",
      "Downloading Natural History Museum of London\n",
      "Downloading Libreria Acqua Alta\n",
      "Downloading Crossbones Graveyard\n",
      "Downloading Pier 54: The Titanic's Arrival Destination\n",
      "Downloading Cathedral of Our Lady of the Angels\n",
      "Downloading Bok Tower Gardens\n",
      "Downloading Morning Glory Pool\n",
      "Downloading The Mail Rail\n",
      "Downloading Canyons of the Ancients\n",
      "Downloading Garden of Fragrance\n",
      "Downloading Museum of the American Gangster\n",
      "Downloading Tiki-Ti \n",
      "Downloading The Sherlock Holmes Museum\n",
      "Downloading Bisti Badlands\n",
      "Downloading Philosophical Research Society\n",
      "Downloading The Domes\n",
      "Downloading American Sign Museum\n",
      "Downloading Eilean Donan \n",
      "Downloading Desert Reef Hot Springs\n",
      "Downloading De Poezenboot (The Cat Boat)\n",
      "Downloading Carousel Bar\n",
      "Downloading Shit Fountain\n",
      "Downloading Garden of Cosmic Speculation\n",
      "Downloading Linda Vista Hospital\n",
      "Downloading Cathedral of Junk\n",
      "Downloading Anata No Warehouse\n",
      "Downloading Kabukicho Robot Restaurant\n",
      "Downloading Neptune Memorial Reef\n",
      "Downloading Quartiere Coppedè\n",
      "Downloading The Dupont Underground\n",
      "Downloading Mystery Castle\n",
      "Downloading The Mansion on O Street\n",
      "Downloading Casa Batlló\n",
      "Downloading Greenacre Park\n",
      "Downloading Mescaline Grove\n",
      "Downloading The Violet Hour\n",
      "Downloading Crystal Cave\n",
      "Downloading New York Transit Museum\n",
      "Downloading La Brea Tar Pits Dragonfly Fossils\n",
      "Downloading Proxy Falls\n",
      "Downloading Carlsbad Caverns\n",
      "Downloading The Sailing Stones of Racetrack Playa\n",
      "Downloading Hellam Township\n",
      "Downloading Linger Eatuary \n",
      "Downloading Pergamon Museum \n",
      "Downloading The King's Cross Ice Well\n",
      "Downloading 'A Sound Garden'\n",
      "Downloading Court of Mysteries\n",
      "Downloading Grand Central Terminal Whispering Gallery \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Devils Postpile National Monument\n",
      "Downloading Cryptozoology & Paranormal Museum\n",
      "Downloading Cathedral Park\n",
      "Downloading Salvador Dalí Museum\n",
      "Downloading Seattle Meowtropolitan Cat Café\n",
      "Downloading Homestead Crater\n",
      "Downloading Salvation Mountain\n",
      "Downloading Museum of the History of Medicine\n",
      "Downloading Unclaimed Baggage Center\n",
      "Downloading Tom's Restaurant\n",
      "Downloading The Long Room Library at Trinity College\n",
      "Downloading Bagby Hot Springs\n",
      "Downloading Sagrada Família\n",
      "Downloading Mammoth Cave\n",
      "Downloading Camera Obscura & World of Illusions\n",
      "Downloading The Museum of Witchcraft & Magic\n",
      "Downloading The Tree of Life\n",
      "Downloading Meteor Crater\n",
      "Downloading St. Mary's College\n",
      "Downloading Sedlec Ossuary \"Bone Church\"\n",
      "ERROR [Errno 22] Invalid argument: 'datas/htmls/Sedlec Ossuary \"Bone Church\".html'\n",
      "Downloading Holyrood Abbey Ruins\n",
      "Downloading Galleria Sciarra\n",
      "Downloading Neversink Pit\n",
      "Downloading Fallingwater\n",
      "Downloading Brooklyn Botanic Garden\n",
      "Downloading Camera Obscura & Holograph Collection\n",
      "Downloading Urban Light\n",
      "Downloading The 'Evil Dead' Cabin\n",
      "Downloading The Museum of Icelandic Sorcery & Witchcraft\n",
      "Downloading The Ape Cave\n",
      "Downloading The Torture Museum\n",
      "Downloading Paris Point Zero\n",
      "Downloading Casa Bonita\n",
      "Downloading Roosevelt Island Tramway\n",
      "Downloading Österreichische Nationalbibliothek (Austrian National Library)\n",
      "Downloading Pinball Hall of Fame\n",
      "Downloading The Tonga Room\n",
      "Downloading The Haserot Angel\n",
      "Downloading National Bonsai Museum\n",
      "Downloading Yoda Fountain\n",
      "Downloading Musical Highway\n",
      "Downloading Fort Worth Water Gardens\n",
      "Downloading Witley Wonder Underwater Ballroom\n",
      "Downloading Prague Astronomical Clock \n",
      "Downloading Rhyolite Ghost Town\n",
      "Downloading Kaaterskill Falls and the Bayard of Dogs\n",
      "Downloading Pink's Hot Dogs\n",
      "Downloading The Witches' Well \n",
      "Downloading Stonehenge\n",
      "Downloading Market Street Catacombs\n",
      "Downloading Aokigahara Forest\n",
      "Downloading George Peabody Library\n",
      "Downloading Forest Haven Asylum \n",
      "Downloading Idiom Installation\n",
      "Downloading Buckland Museum of Witchcraft and Magick\n",
      "Downloading The Natural Bridge\n",
      "Downloading Bishop Castle\n",
      "Downloading The Sunken City of Baia\n",
      "Downloading Pando, the Trembling Giant\n",
      "Downloading Grand Prismatic Spring\n",
      "Downloading Psychiatry: An Industry of Death Museum \n",
      "Downloading The Room of Endangered and Extinct Species\n",
      "Downloading Sunny Jim Cave Store\n",
      "Downloading Uncommon Objects\n",
      "Downloading Traffic Light Tree\n",
      "Downloading Babycastles\n",
      "Downloading The London Dungeon\n",
      "Downloading Lummis Home (\"El Alisal\")\n",
      "ERROR [Errno 22] Invalid argument: 'datas/htmls/Lummis Home (\"El Alisal\").html'\n",
      "Downloading Forbidden Caverns\n",
      "Downloading Sunshine Laundromat\n",
      "Downloading Octopus Tree of Oregon\n",
      "Downloading The Center of the Universe\n",
      "Downloading Salton Sea\n",
      "Downloading Dripstone Wall\n",
      "Downloading Garden of the Gods\n",
      "Downloading Alice in Wonderland Statue\n",
      "Downloading Grant Museum of Zoology\n",
      "Downloading Castle Frankenstein\n",
      "Downloading Blue Ghost Fireflies\n",
      "Downloading The Giant Ghibli Clock\n",
      "Downloading Kubota Garden\n",
      "Downloading Le Boudoir\n",
      "Downloading Los Feliz Murder Mansion\n",
      "Downloading Santa Cruz Mystery Spot\n",
      "Downloading Myopic Books\n",
      "Downloading Treehouse Point\n",
      "Downloading International Museum of Surgical Science\n",
      "Downloading Trundle Manor: House of Oddities\n",
      "Downloading Culture House\n",
      "Downloading Speculum Alchemiae\n",
      "Downloading The Magic Castle\n",
      "Downloading Ruins of the Sutro Baths\n",
      "Downloading KattenKabinet\n",
      "Downloading Design Panoptikum\n",
      "Downloading Trinity Churchyard\n",
      "Downloading Labyrinth Park of Horta\n",
      "Downloading CDC Museum\n",
      "Downloading Victoria Beach's Pirate Tower\n",
      "Downloading Hortus Botanicus Amsterdam\n",
      "Downloading New Orleans Pharmacy Museum\n",
      "Downloading The Old Curiosity Shop\n",
      "Downloading Greenwich Foot Tunnel\n",
      "Downloading Junkman's Daughter\n",
      "Downloading Nonnas of the World\n",
      "Downloading Devil's Den\n",
      "Downloading Quinta da Regaleira\n",
      "Downloading Seven Noses of Soho\n",
      "Downloading Hess Triangle\n",
      "Downloading Seattle Pinball Museum\n",
      "Downloading The Dark Hedges\n",
      "Downloading Séance Room at Muriel's Jackson Square\n",
      "Downloading Lafayette Cemetery\n",
      "Downloading New York Federal Gold Vault\n",
      "Downloading Hidden Beach\n",
      "Downloading Caverns Grotto\n",
      "Downloading Forestiere Underground Gardens\n",
      "Downloading Danvers State Hospital\n",
      "Downloading The Royal Botanic Gardens of Edinburgh\n",
      "Downloading LaLaurie Mansion \n",
      "Downloading Exploratorium\n",
      "Downloading Hvítserkur\n",
      "Downloading Spectre Set Ruins\n",
      "Downloading Fluorescent Rocks of Sterling Hill Mine\n",
      "Downloading Orfield Labs Quiet Chamber\n",
      "Downloading Growing Underground\n",
      "Downloading The Monkey's Paw\n",
      "Downloading Ruby Falls\n",
      "Downloading Madonna Inn\n",
      "Downloading Temple of Mithras\n",
      "Downloading Temple Church\n",
      "Downloading Tiffany Glass Mural \"The Dream Garden\"\n",
      "ERROR [Errno 22] Invalid argument: 'datas/htmls/Tiffany Glass Mural \"The Dream Garden\".html'\n",
      "Downloading Wellcome Collection & Library\n",
      "Downloading The Dome Illusion\n",
      "Downloading Museum Vrolik\n",
      "Downloading Voodoo Doughnut and Wedding Chapel\n",
      "Downloading Venice of America Canals\n",
      "Downloading Nunhead Cemetery\n",
      "Downloading Un Regard Moderne Bookstore\n",
      "Downloading Weeki Wachee: City of Live Mermaids\n",
      "Downloading Porta Alchemica\n",
      "Downloading Valley Relics Museum\n",
      "Downloading Leake Street Graffiti Tunnel\n",
      "Downloading Campbell Apartment\n",
      "Downloading General Sherman \n",
      "Downloading Shiratani Unsuikyo Ravine\n",
      "Downloading Sanrio Puroland\n",
      "Downloading The Great Serpent Mound\n",
      "Downloading Antelope Valley Poppy Reserve\n",
      "Downloading Bryce Canyon\n"
     ]
    }
   ],
   "source": [
    "class htmlDownloader():\n",
    "    def __init__(self,file_path:str=\"datas/urls.csv\") -> None:\n",
    "        self.urls = self._url_reader(file_path)\n",
    "\n",
    "    def _url_reader(self,path:str,format_file=\"csv\") -> pd.DataFrame :\n",
    "        if format_file ==\"txt\" :\n",
    "            return pd.read_csv(path,sep=\" \")\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "    #if program stops, to not lose the data that collected up to the stopping point.\n",
    "    def download_htmls(self) -> None :\n",
    "        config = self.last_download_control()\n",
    "        roi_urls = self.urls.iloc[config[\"last_downloaded\"]:]\n",
    "        for idx,row in roi_urls.iterrows():\n",
    "            try:\n",
    "                print(f\"Downloading {row.NAME}\")\n",
    "                sleep(random.randint(0,2))\n",
    "                htmls = requests.get(row.URL).content\n",
    "                saving_path = f\"datas/htmls/{row.NAME}.html\"\n",
    "                with open(saving_path,\"wb\") as f:\n",
    "                    f.write(htmls)\n",
    "                config[\"last_downloaded\"] += 1 \n",
    "\n",
    "                if idx % 10 == 0 :\n",
    "                    self.save_config_file(config)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR {e}\")\n",
    "                self.save_config_file(config=config)\n",
    "                    \n",
    "\n",
    "    #if program stops, to not lose last data that collected up to the stopping point.\n",
    "    def last_download_control(self) -> Dict[str,int]:\n",
    "\n",
    "        if os.path.isfile(\"config.json\"):\n",
    "            with open(\"config.json\",\"rb\") as f :\n",
    "                config = json.load(f)\n",
    "        else:\n",
    "            config = {\"last_downloaded\":0}\n",
    "            with open(\"config.json\",\"w\") as f:\n",
    "                json.dump(config,f)\n",
    "        return config\n",
    "\n",
    "    def save_config_file(self,config:Dict[str,int]):\n",
    "        with open(\"config.json\",\"w\") as f:\n",
    "            json.dump(config,f)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    downloader = htmlDownloader()\n",
    "    downloader.download_htmls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab0be5",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de183995",
   "metadata": {},
   "source": [
    "First we defined places of interest that given us. \n",
    "Then for each place, we created a place_i.tsv file according to given format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daa48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class htmlParser():\n",
    "    HTML_FILES = \"datas/htmls/\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.html_files = os.listdir(htmlParser.HTML_FILES)\n",
    "        self.dataframe = pd.read_csv(\"datas/urls.csv\")\n",
    "        self.dataframe.NAME = self.dataframe.NAME.str.strip()\n",
    "        self.process_pages()\n",
    "\n",
    "\n",
    "    def read_file(self,file_name:str) -> bs4.BeautifulSoup:\n",
    "        file_path = htmlParser.HTML_FILES + file_name\n",
    "\n",
    "        with open(file_path,\"r\") as f:\n",
    "            html = f.read()\n",
    "        return bs4.BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "\n",
    "    def extract_header_information(self,soup:bs4.BeautifulSoup):\n",
    "        roi_header = soup.find_all(\"div\", {\"class\": \"DDPage__header-container grid-row\"})[0]\n",
    "        self.placeName = roi_header.find(\"h1\", {\"class\":\"DDPage__header-title\"}).text.strip()\n",
    "\n",
    "        location = roi_header.find(\"div\", {\"class\":\"DDPage__header-place-location\"}).text.strip()\n",
    "\n",
    "        counters = roi_header.find_all(\"div\",{\"class\":\"title-md item-action-count\"})\n",
    "        self.numPeopleVisited,self.numPeopleWant = int(counters[0].text.strip()),int(counters[1].text.strip())\n",
    "        self.placeShortDesc = roi_header.find(\"h3\", {\"class\":\"DDPage__header-dek\"}).text.strip()\n",
    "        # set_trace(header=\"header infos\")\n",
    "    \n",
    "    def extract_descriptions(self,soup:bs4.BeautifulSoup):\n",
    "\n",
    "        ## Main Descriptions \n",
    "        placeDesc = soup.find_all(\"div\", {\"class\": \"DDP__body-copy\"})[0]\n",
    "        self.placeDesc = \"\".join([p.text.strip() for p in placeDesc.find_all(\"p\")])\n",
    "        # set_trace(header=\"descriptions\")\n",
    "    \n",
    "    def extract_sidebar(self,soup:bs4.BeautifulSoup):\n",
    "        sidebar = soup.find(\"div\", {\"class\":\"DDPageSiderail\"})\n",
    "        nearby_places = sidebar.find_all(\"div\",{\"class\":\"DDPageSiderailRecirc__item-title\"})\n",
    "        positions =  sidebar.find(\"div\",{\"class\":\"DDPageSiderail__coordinates js-copy-coordinates\"}).attrs[\"data-coordinates\"].split(\",\")\n",
    "        \n",
    "        #9-Latitud and Longitude of the place's location,  #7-placeNearby,  8-placeAddress\n",
    "        self.placeAlt, self.placeLong = float(positions[0].strip()), float(positions[1].strip())\n",
    "        self.placeNearby = [place.text for place in nearby_places]\n",
    "        self.placeAddress =  sidebar.find(\"address\",{\"class\":\"DDPageSiderail__address\"}).find(\"div\",recursive=False).get_text(\" \").replace(\"\\n\",\"\")\n",
    "\n",
    "    def extract_footer(self,soup:bs4.BeautifulSoup):\n",
    "        footer = soup.find(\"div\", {\"id\":\"ugc-module\"})\n",
    "        Editors = footer.find_all(\"a\",{\"class\":\"DDPContributorsList__contributor\"})\n",
    "        placePubDate = footer.find(\"div\",{\"class\":\"DDPContributor__name\"}).text\n",
    "        #10-placeEditors, 11-placePubDate, 2-placeTags\n",
    "        self.placeEditors = [editors.find(\"span\").text if editors.find(\"span\") else editors.text for editors in Editors]\n",
    "        self.placePubDate = datetime.strptime(placePubDate,\"%B %d, %Y\")\n",
    "        self.placeTags = [item.get_text(\"\").replace(\"\\n\",\"\") for item in soup.find_all(\"a\",{\"class\":\"itemTags__link js-item-tags-link\"})]\n",
    "        # set_trace(header=\"sidebar\")\n",
    "\n",
    "    def extract_related_places(self,soup:bs4.BeautifulSoup):\n",
    "        \n",
    "        related_list = soup.find(\"div\",{\n",
    "            \"class\":\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\",\n",
    "            \"data-gtm-template\":\"DDP Footer Recirc Lists\"\n",
    "        })\n",
    "        related_places = soup.find(\"div\",{\n",
    "            \"class\":\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\",\n",
    "            \"data-gtm-template\":\"DDP Footer Recirc Related\"\n",
    "        })\n",
    "        #12-placeRelatedLists,   #13-placeRelatedPlaces\n",
    "        self.placeRelatedLists = [data.find(\"span\").text.strip() for data in related_list.find_all(\"h3\")]\n",
    "        self.placeRelatedPlaces = [data.find(\"span\").text.strip() for  data in related_places.find_all(\"h3\")]\n",
    "        \n",
    "    def process_pages(self):\n",
    "        \n",
    "        for idx,file_name in enumerate(self.html_files):\n",
    "            try:\n",
    "                soup = self.read_file(file_name=file_name)\n",
    "                self.extract_header_information(soup=soup)\n",
    "                self.extract_descriptions(soup=soup)\n",
    "                self.extract_sidebar(soup=soup)\n",
    "                self.extract_footer(soup=soup)\n",
    "                self.extract_related_places(soup=soup)\n",
    "                #14-placeUrl\n",
    "                self.placeUrl = self.dataframe.loc[self.dataframe.NAME == self.placeName][\"URL\"]\n",
    "                self.save_tsv_file(idx)\n",
    "            except Exception as e : \n",
    "                print(e)\n",
    "                set_trace() #debuggig.\n",
    "                \n",
    "    def save_tsv_file(self,idx:int):\n",
    "        with open(f\"datas/tsv_files/place_{idx}.tsv\",\"w\") as f:\n",
    "            whole_data = [\n",
    "                self.placeName,str(self.placeTags),str(self.numPeopleVisited),\n",
    "                str(self.numPeopleWant),self.placeDesc,self.placeShortDesc,\n",
    "                str(self.placeNearby),self.placeAddress,str(self.placeAlt),\n",
    "                str(self.placeLong),str(self.placeEditors),self.placePubDate.strftime(\"%Y-%m-%d\"),\n",
    "                str(self.placeRelatedLists),\n",
    "                str(self.placeRelatedPlaces),self.placeUrl.values[0]]\n",
    "            \n",
    "            tsv = \"\\t\".join(whole_data)\n",
    "            f.write(tsv)\n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    htmlParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a19d9b2",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bba640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e65250be",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952459da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7421e332",
   "metadata": {},
   "source": [
    "###  2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d9d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72569fb0",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cf27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259c992b",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eba25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98cd60c5",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca464d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca1a0a6",
   "metadata": {},
   "source": [
    "### 2.2.1) Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236de9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6cb2ea",
   "metadata": {},
   "source": [
    "### 2.2.2) Execute the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ca8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58e8ddd5",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b17b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94819b95",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da3ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dbd300d",
   "metadata": {},
   "source": [
    "## 5. BONUS: More complex search engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25971b60",
   "metadata": {},
   "source": [
    "## 6. Command line question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bafc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3740e55a",
   "metadata": {},
   "source": [
    "## 7. Theoretical question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
