{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc88ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5bdae",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb41fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2='https://www.atlasobscura.com/places?page='\n",
    "sort='&sort=likes_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d360f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS!\n",
    "# create a file to put all the urls\n",
    "f = open(\"urlpages.txt\", \"w\")\n",
    "for page in range(1, 401):\n",
    "    url = url2 +str(page) +sort\n",
    "    page_ = requests.get(url)\n",
    "    soup = BeautifulSoup(page_.content, features='lxml')\n",
    "    for a in soup.find_all('a', class_=\"content-card content-card-place\"): #the class where the url is\n",
    "        f.write(a.get('href')+'\\n')\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916eabc",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5c08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "    pathAncestor = os.path.join(\"./\", \"htmlpages\") # Path\n",
    "    #os.mkdir(pathAncestor) # create the folder in the path\n",
    "    \n",
    "    f = open(\"urlpages2.txt\", \"w\") #create txt file with urls\n",
    "    count=1728 #counting html files\n",
    "    headpart='https://www.atlasobscura.com'\n",
    "    \n",
    "    for i in range(116, 125):\n",
    "        os.makedirs(os.path.join(pathAncestor, 'page ' + str(i))) #create folder of html files\n",
    "        \n",
    "        url = url2 +str(i) +sort\n",
    "        page_1 = requests.get(url)\n",
    "        soup = BeautifulSoup(page_1.content, features='lxml')\n",
    "        \n",
    "        for a in soup.find_all('a', class_=\"content-card content-card-place\"): #the class where the url is\n",
    "            save=a.get('href')\n",
    "            f.write(save +'\\n')\n",
    "            \n",
    "            subdirectory = pathAncestor + \"/page \" + str(i) # select the corresponding folder to insert the html article\n",
    "            article_name = \"/article_\"+str(count)+\".html\" \n",
    "            count+=1\n",
    "            complete_path = subdirectory + article_name # insert the new complete path where create the html file\n",
    "            \n",
    "            with open(complete_path, \"wb\") as ip_file:\n",
    "                link = headpart + save\n",
    "                try:\n",
    "                    page_2 = requests.get(link) # request the page\n",
    "                except:\n",
    "                    with open(\"failureRequest.txt\", \"a\") as err_file: # if we loose the request place, we put into a file the link doesn't download well, then we set the \"urlpages.txt\" with these link\n",
    "                        err_file.write(link)\n",
    "                        err_file.close()\n",
    "\n",
    "                soup = BeautifulSoup(page_2.text, features='lxml')\n",
    "\n",
    "                ip_file.write(soup.encode('utf-8'))\n",
    "                ip_file.close()\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301389f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31923ba1",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b6a24",
   "metadata": {},
   "source": [
    "We create the tsv files for each html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c10f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(tsv_writer, article): \n",
    "    \n",
    "    with open(article, 'r', encoding=\"utf-8\") as out_file: # for each html article downloaded scrape it!\n",
    "        contents = out_file.read()\n",
    "        soup = BeautifulSoup(contents, features=\"lxml\") #parse the text\n",
    "        \n",
    "    try:\n",
    "        roi_header = soup.find_all(\"div\", {\"class\": \"DDPage__header-container grid-row\"})[0]\n",
    "        placeName = roi_header.find(\"h1\", {\"class\":\"DDPage__header-title\"}).text.strip()\n",
    "\n",
    "    except:\n",
    "        placeName = \" \"\n",
    "    try:\n",
    "        related_list = soup.find(\"div\",{\n",
    "            \"class\":\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\",\n",
    "            \"data-gtm-template\":\"DDP Footer Recirc Lists\"\n",
    "        })\n",
    "        \n",
    "        placeRelatedLists = [data.find(\"span\").text.strip() for data in related_list.find_all(\"h3\")]\n",
    "    except:\n",
    "        placeRelatedLists=''\n",
    "        \n",
    "    try:\n",
    "        related_places = soup.find(\"div\",{\n",
    "            \"class\":\"card-grid CardRecircSection__card-grid js-inject-gtm-data-in-child-links\",\n",
    "            \"data-gtm-template\":\"DDP Footer Recirc Related\"\n",
    "        })\n",
    "        placeRelatedPlaces = [data.find(\"span\").text.strip() for  data in related_places.find_all(\"h3\")]\n",
    "    except:\n",
    "        placeRelatedPlaces=''\n",
    "    try:\n",
    "        placeDesc = soup.find_all(\"div\", {\"class\": \"DDP__body-copy\"})[0]\n",
    "        placeDesc = \"\".join([p.text.strip() for p in placeDesc.find_all(\"p\")])\n",
    "    except:\n",
    "        placeDesc=''\n",
    "    try:\n",
    "        location = roi_header.find(\"div\", {\"class\":\"DDPage__header-place-location\"}).text.strip()\n",
    "    except:\n",
    "        location=''\n",
    "    try:\n",
    "        counters = roi_header.find_all(\"div\",{\"class\":\"title-md item-action-count\"})\n",
    "        numPeopleVisited=int(counters[0].text.strip())\n",
    "    except:\n",
    "        numPeopleVisited=''\n",
    "    try:\n",
    "        numPeopleWant = int(counters[1].text.strip())\n",
    "    except:\n",
    "        numPeopleWant=''\n",
    "    try:\n",
    "        placeShortDesc = roi_header.find(\"h3\", {\"class\":\"DDPage__header-dek\"}).text.strip()\n",
    "    except:\n",
    "        placeShortDesc=''\n",
    "    try:\n",
    "        sidebar = soup.find(\"div\", {\"class\":\"DDPageSiderail\"})\n",
    "        nearby_places = sidebar.find_all(\"div\",{\"class\":\"DDPageSiderailRecirc__item-title\"})\n",
    "        positions =  sidebar.find(\"div\",{\"class\":\"DDPageSiderail__coordinates js-copy-coordinates\"}).attrs[\"data-coordinates\"].split(\",\")\n",
    "        placeAlt= float(positions[0].strip())\n",
    "    except:\n",
    "        placeAlt=''\n",
    "    try:\n",
    "        placeLong = float(positions[1].strip())\n",
    "    except:\n",
    "        placeLong=''\n",
    "    try:\n",
    "        placeNearby = [place.text for place in nearby_places]\n",
    "    except:\n",
    "        placeNearby=''\n",
    "    try:\n",
    "        placeAddress = [x.text for x in soup.find_all('div', {'class': 'DDPage__header-place-location'})][0]\n",
    "        \n",
    "    except:\n",
    "        placeAddress=''\n",
    "        \n",
    "    try:\n",
    "        footer = soup.find(\"div\", {\"id\":\"ugc-module\"})\n",
    "        Editors = footer.find_all(\"a\",{\"class\":\"DDPContributorsList__contributor\"})\n",
    "        placePubDate = footer.find(\"div\",{\"class\":\"DDPContributor__name\"}).text\n",
    "        placePubDate = datetime.strptime(placePubDate,\"%B %d, %Y\")\n",
    "    except:\n",
    "        placePubDate=''\n",
    "    try:\n",
    "        #10-placeEditors, 11-placePubDate, 2-placeTags\n",
    "        placeEditors = [editors.find(\"span\").text if editors.find(\"span\") else editors.text for editors in Editors]\n",
    "    except:\n",
    "        placeEditors=''\n",
    "    try:\n",
    "        placeTags = [item.get_text(\"\").replace(\"\\n\",\"\") for item in soup.find_all(\"a\",{\"class\":\"itemTags__link js-item-tags-link\"})]\n",
    "        # set_trace(header=\"sidebar\")\n",
    "    except:   \n",
    "        placeTags=''\n",
    "\n",
    "    placeURL=''\n",
    "\n",
    "    tsv_writer.writerow([placeName, placeTags, numPeopleVisited, numPeopleWant, \n",
    "                                placeDesc, placeShortDesc, placeNearby, placeAddress, placeAlt,\n",
    "                                placeLong, placeEditors, placePubDate,placeRelatedLists,placeRelatedPlaces,placeURL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17d75403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tsv(input_path):\n",
    "    path = str(\"./\"+input_path)\n",
    "\n",
    "    filenames = os.listdir(path)\n",
    "    for i in range(116, 125):\n",
    "        filenames = os.listdir(path + '/page ' + str(i))\n",
    "\n",
    "        for file in filenames:\n",
    "            with open(path + '/page ' + str(i) + '/article_'+str(file.split(\"_\")[1].replace(\".html\", \"\"))+'.tsv', 'w', encoding=\"utf-8\", newline='') as out_file: # create for each html article its article_i.tsv \n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                tsv_writer.writerow(['placeName', 'placeTags', 'numPeopleVisited', 'numPeopleWant', \n",
    "                                'placeDesc', 'placeShortDesc', 'placeNearby', 'placeAddress', 'placeAlt',\n",
    "                                'placeLong', 'placeEditors', 'placePubDate','placeRelatedLists','placeRelatedPlaces','placeURL'])\n",
    "                scrap(tsv_writer, path + '/page ' + str(i) + \"/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6fd6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tsv('htmlpages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45f999",
   "metadata": {},
   "source": [
    "We create the tsv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7800df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv():\n",
    "    path = \"./htmlpages\"\n",
    "    data_f = pd.DataFrame()\n",
    "\n",
    "    for num in range(1, 401):\n",
    "        filenames = os.listdir(path + '/page ' + str(num))\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".tsv\"): # look for tsv files\n",
    "                with open(path + '/page ' + str(num) + '/article_'+str(file.split(\"_\")[1]), 'r', encoding=\"utf-8\", newline='') as file_tsv:\n",
    "                        df = pd.read_csv(file_tsv,sep = \"\\t\")\n",
    "                        data_f = pd.concat([data_f,df])\n",
    "                            \n",
    "    with open(\"data.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as text_file: \n",
    "        text_file.write(data_f.to_csv(index=False)) # save the new file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3c51f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv() #we call the tsv function to crete our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1fb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/home/sophja/Documenti/ADM/HW3/data.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd631047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placeName</th>\n",
       "      <th>placeTags</th>\n",
       "      <th>numPeopleVisited</th>\n",
       "      <th>numPeopleWant</th>\n",
       "      <th>placeDesc</th>\n",
       "      <th>placeShortDesc</th>\n",
       "      <th>placeNearby</th>\n",
       "      <th>placeAddress</th>\n",
       "      <th>placeAlt</th>\n",
       "      <th>placeLong</th>\n",
       "      <th>placeEditors</th>\n",
       "      <th>placePubDate</th>\n",
       "      <th>placeRelatedLists</th>\n",
       "      <th>placeRelatedPlaces</th>\n",
       "      <th>placeURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Catacombes de Paris</td>\n",
       "      <td>['ossuaries', 'memento mori', 'catacombs and c...</td>\n",
       "      <td>4457.0</td>\n",
       "      <td>7059.0</td>\n",
       "      <td>In 2004, Parisian police were assigned to do a...</td>\n",
       "      <td>The vast, legendary catacombs hold secrets muc...</td>\n",
       "      <td>['Sculptures de Décure', 'Arago Medallions', \"...</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>48.8343</td>\n",
       "      <td>2.3322</td>\n",
       "      <td>['CPilgrim', 'ack sed', 'ramonrodz2212', 'ging...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['19 Catacombs Sure to Tingle Your Spine', \"Th...</td>\n",
       "      <td>['Ossario di San Martino', 'Leuk Charnel House...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dream House</td>\n",
       "      <td>['optical oddities', 'eccentric homes', 'music...</td>\n",
       "      <td>766.0</td>\n",
       "      <td>6007.0</td>\n",
       "      <td>When walking down Church Street in Tribeca, ke...</td>\n",
       "      <td>La Monte Young and Marian Zazeela's \"Dream Hou...</td>\n",
       "      <td>['Aretha Franklin Subway Tributes', 'Farm.One'...</td>\n",
       "      <td>Manhattan, New York</td>\n",
       "      <td>40.7185</td>\n",
       "      <td>-74.0048</td>\n",
       "      <td>['seanmattison', 'erosika', 'mbison', 'Gray', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['30 Unexpected Places to Have a Joyful Advent...</td>\n",
       "      <td>['Callejon de Hamel', 'Casa Neverlandia', 'Cas...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>City Hall Station</td>\n",
       "      <td>['subways', 'subterranean', 'infrastructure', ...</td>\n",
       "      <td>1834.0</td>\n",
       "      <td>8608.0</td>\n",
       "      <td>The first New York City subway was built and o...</td>\n",
       "      <td>A beautiful and abandoned New York subway stat...</td>\n",
       "      <td>['African Burial Ground National Monument', 'T...</td>\n",
       "      <td>Manhattan, New York</td>\n",
       "      <td>40.7134</td>\n",
       "      <td>-74.0046</td>\n",
       "      <td>['Rebekah Otto', 'charding407', 'mbison', 'Ann...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['30 Unexpected Places to Have a Joyful Advent...</td>\n",
       "      <td>['Crystal Palace Subway', 'Moscow Metro Statio...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Evolution Store</td>\n",
       "      <td>['wonder cabinets', 'purveyors of curiosities'...</td>\n",
       "      <td>2199.0</td>\n",
       "      <td>6953.0</td>\n",
       "      <td>Evolution stands out among the clothing stores...</td>\n",
       "      <td>A terrific purveyor of natural history objects...</td>\n",
       "      <td>[\"Merchant's House Museum\", 'The Brown Buildin...</td>\n",
       "      <td>Manhattan, New York</td>\n",
       "      <td>40.7281</td>\n",
       "      <td>-73.9948</td>\n",
       "      <td>['mbison', 'ramosju', 'Lauren Levesque', 'Unus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"The World's Top 100 Wonders in 2018\", \"New Y...</td>\n",
       "      <td>['Kunstkammer Georg Laue', 'Ospedale delle Bam...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Winchester Mystery House</td>\n",
       "      <td>['follies and grottoes', 'outsider architectur...</td>\n",
       "      <td>3724.0</td>\n",
       "      <td>5208.0</td>\n",
       "      <td>In 1886 an eccentric woman named Sarah Winches...</td>\n",
       "      <td>A peculiar mansion built by the troubled heir ...</td>\n",
       "      <td>['Santana Row Chess Plaza', 'Rosicrucian Park'...</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>37.3189</td>\n",
       "      <td>-121.9506</td>\n",
       "      <td>['mbison', 'Avoiding Regret', 'Collin', 'marle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['12 Places to See the Work of Women Architect...</td>\n",
       "      <td>['Casa de Piedra (Stone House)', 'Portmeirion ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      placeName  \\\n",
       "0           Catacombes de Paris   \n",
       "1                   Dream House   \n",
       "2             City Hall Station   \n",
       "3           The Evolution Store   \n",
       "4  The Winchester Mystery House   \n",
       "\n",
       "                                           placeTags  numPeopleVisited  \\\n",
       "0  ['ossuaries', 'memento mori', 'catacombs and c...            4457.0   \n",
       "1  ['optical oddities', 'eccentric homes', 'music...             766.0   \n",
       "2  ['subways', 'subterranean', 'infrastructure', ...            1834.0   \n",
       "3  ['wonder cabinets', 'purveyors of curiosities'...            2199.0   \n",
       "4  ['follies and grottoes', 'outsider architectur...            3724.0   \n",
       "\n",
       "   numPeopleWant                                          placeDesc  \\\n",
       "0         7059.0  In 2004, Parisian police were assigned to do a...   \n",
       "1         6007.0  When walking down Church Street in Tribeca, ke...   \n",
       "2         8608.0  The first New York City subway was built and o...   \n",
       "3         6953.0  Evolution stands out among the clothing stores...   \n",
       "4         5208.0  In 1886 an eccentric woman named Sarah Winches...   \n",
       "\n",
       "                                      placeShortDesc  \\\n",
       "0  The vast, legendary catacombs hold secrets muc...   \n",
       "1  La Monte Young and Marian Zazeela's \"Dream Hou...   \n",
       "2  A beautiful and abandoned New York subway stat...   \n",
       "3  A terrific purveyor of natural history objects...   \n",
       "4  A peculiar mansion built by the troubled heir ...   \n",
       "\n",
       "                                         placeNearby          placeAddress  \\\n",
       "0  ['Sculptures de Décure', 'Arago Medallions', \"...         Paris, France   \n",
       "1  ['Aretha Franklin Subway Tributes', 'Farm.One'...   Manhattan, New York   \n",
       "2  ['African Burial Ground National Monument', 'T...   Manhattan, New York   \n",
       "3  [\"Merchant's House Museum\", 'The Brown Buildin...   Manhattan, New York   \n",
       "4  ['Santana Row Chess Plaza', 'Rosicrucian Park'...  San Jose, California   \n",
       "\n",
       "   placeAlt  placeLong                                       placeEditors  \\\n",
       "0   48.8343     2.3322  ['CPilgrim', 'ack sed', 'ramonrodz2212', 'ging...   \n",
       "1   40.7185   -74.0048  ['seanmattison', 'erosika', 'mbison', 'Gray', ...   \n",
       "2   40.7134   -74.0046  ['Rebekah Otto', 'charding407', 'mbison', 'Ann...   \n",
       "3   40.7281   -73.9948  ['mbison', 'ramosju', 'Lauren Levesque', 'Unus...   \n",
       "4   37.3189  -121.9506  ['mbison', 'Avoiding Regret', 'Collin', 'marle...   \n",
       "\n",
       "   placePubDate                                  placeRelatedLists  \\\n",
       "0           NaN  ['19 Catacombs Sure to Tingle Your Spine', \"Th...   \n",
       "1           NaN  ['30 Unexpected Places to Have a Joyful Advent...   \n",
       "2           NaN  ['30 Unexpected Places to Have a Joyful Advent...   \n",
       "3           NaN  [\"The World's Top 100 Wonders in 2018\", \"New Y...   \n",
       "4           NaN  ['12 Places to See the Work of Women Architect...   \n",
       "\n",
       "                                  placeRelatedPlaces  placeURL  \n",
       "0  ['Ossario di San Martino', 'Leuk Charnel House...       NaN  \n",
       "1  ['Callejon de Hamel', 'Casa Neverlandia', 'Cas...       NaN  \n",
       "2  ['Crystal Palace Subway', 'Moscow Metro Statio...       NaN  \n",
       "3  ['Kunstkammer Georg Laue', 'Ospedale delle Bam...       NaN  \n",
       "4  ['Casa de Piedra (Stone House)', 'Portmeirion ...       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75300a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7198 entries, 0 to 7197\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   placeName           7198 non-null   object \n",
      " 1   placeTags           7198 non-null   object \n",
      " 2   numPeopleVisited    7193 non-null   float64\n",
      " 3   numPeopleWant       7193 non-null   float64\n",
      " 4   placeDesc           7193 non-null   object \n",
      " 5   placeShortDesc      7193 non-null   object \n",
      " 6   placeNearby         7193 non-null   object \n",
      " 7   placeAddress        7192 non-null   object \n",
      " 8   placeAlt            7193 non-null   float64\n",
      " 9   placeLong           7193 non-null   float64\n",
      " 10  placeEditors        7193 non-null   object \n",
      " 11  placePubDate        0 non-null      float64\n",
      " 12  placeRelatedLists   2759 non-null   object \n",
      " 13  placeRelatedPlaces  7159 non-null   object \n",
      " 14  placeURL            0 non-null      float64\n",
      "dtypes: float64(6), object(9)\n",
      "memory usage: 843.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf77f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
